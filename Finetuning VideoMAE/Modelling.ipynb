{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f6dffa",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d616d54",
   "metadata": {},
   "source": [
    "### 1.1 Adjusting the format of `WLASL_v0.3.json`. Also select a subset of 28 glosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c752c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_flat_dataset(wlasl_json_path, videos_dir, output_path):\n",
    "    # Only include these 29 selected glosses\n",
    "    selected_glosses = {\n",
    "        \"cousin\", \"deaf\", \"help\", \"call\", \"give\", \"take\", \"like\", \"laugh\",\n",
    "        \"order\", \"drop\", \"pizza\", \"candy\", \"shirt\", \"room\", \"bar\", \"language\",\n",
    "        \"speech\", \"cool\", \"silly\", \"sweet\", \"careful\", \"thin\", \"last\", \"soon\",\n",
    "        \"what\", \"california\", \"convince\", \"interest\"\n",
    "    }\n",
    "\n",
    "    with open(wlasl_json_path, \"r\") as f:\n",
    "        wlasl_data = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for entry in wlasl_data:\n",
    "        gloss = entry[\"gloss\"]\n",
    "\n",
    "        # Only include glosses from the selected list\n",
    "        if gloss not in selected_glosses:\n",
    "            continue\n",
    "\n",
    "        split = entry.get(\"split\", \"unknown\")\n",
    "\n",
    "        for instance in entry[\"instances\"]:\n",
    "            video_id = instance.get(\"video_id\")\n",
    "            video_filename = f\"{video_id}.mp4\"\n",
    "            video_path = videos_dir / video_filename\n",
    "\n",
    "            # Only include if the actual video file exists\n",
    "            if video_path.exists():\n",
    "                frame_start = instance.get(\"frame_start\")\n",
    "                frame_end = instance.get(\"frame_end\")\n",
    "                instance_split = instance.get(\"split\", split)\n",
    "\n",
    "                # Final video path in output JSON\n",
    "                relative_video_path = f\"data/{video_filename}\"\n",
    "\n",
    "                dataset.append({\n",
    "                    \"gloss\": gloss,\n",
    "                    \"video_path\": relative_video_path,\n",
    "                    \"frame_start\": frame_start,\n",
    "                    \"frame_end\": frame_end,\n",
    "                    \"split\": instance_split\n",
    "                })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb2f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd()\n",
    "wlasl_json_path = base_dir / \"WLASL_v0.3.json\"\n",
    "videos_dir = base_dir / \"Videos\"\n",
    "output_path = base_dir / \"WLASL_parsed_data_adjustedpath.json\"\n",
    "\n",
    "generate_flat_dataset(wlasl_json_path, videos_dir, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f56c4d",
   "metadata": {},
   "source": [
    "### 1.2 Restructure the files into `data/test`, `data/train` and `data/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('WLASL_parsed_data_adjustedpath.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Base directory where the new folders will be created\n",
    "base_dir = 'data'\n",
    "moved_files = 0\n",
    "missing_files = 0\n",
    "\n",
    "for item in data:\n",
    "    # Use the full relative path from JSON (e.g., \"Videos/14894.mp4\")\n",
    "    current_path = os.path.normpath(item['video_path'])\n",
    "\n",
    "    if os.path.exists(current_path):\n",
    "        # Extract metadata\n",
    "        split = item['split']\n",
    "        gloss = item['gloss']\n",
    "\n",
    "        # Create destination directory structure\n",
    "        split_dir = os.path.join(base_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        gloss_dir = os.path.join(split_dir, gloss)\n",
    "        os.makedirs(gloss_dir, exist_ok=True)\n",
    "\n",
    "        # Get just the filename\n",
    "        filename = os.path.basename(current_path)\n",
    "\n",
    "        # Define the new destination path\n",
    "        new_path = os.path.join(gloss_dir, filename)\n",
    "\n",
    "        # Move the file\n",
    "        shutil.move(current_path, new_path)\n",
    "\n",
    "        moved_files += 1\n",
    "        print(f\"The video {current_path} is moved to {new_path}\")\n",
    "    else:\n",
    "        missing_files += 1\n",
    "        print(f\"The video {current_path} does not exist\")\n",
    "\n",
    "print(f\"Moved {moved_files} files and {missing_files} files are missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87a0e1",
   "metadata": {},
   "source": [
    "### 1.3 Display the video distribution of `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ee1f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#    Gloss                Train   Val  Test  Total\n",
      "------------------------------------------------------------\n",
      "1    cool                    11     3     2     16\n",
      "2    thin                    11     3     2     16\n",
      "3    cousin                   9     3     2     14\n",
      "4    help                    10     2     2     14\n",
      "5    candy                    8     2     3     13\n",
      "6    call                     8     2     2     12\n",
      "7    last                     8     2     2     12\n",
      "8    pizza                    8     2     2     12\n",
      "9    shirt                    8     2     2     12\n",
      "10   what                     6     3     3     12\n",
      "11   bar                      6     2     3     11\n",
      "12   deaf                     7     2     2     11\n",
      "13   laugh                    6     3     2     11\n",
      "14   room                     7     2     2     11\n",
      "15   soon                     7     2     2     11\n",
      "16   take                     7     2     2     11\n",
      "17   convince                 6     2     2     10\n",
      "18   give                     6     2     2     10\n",
      "19   interest                 6     2     2     10\n",
      "20   language                 6     2     2     10\n",
      "21   like                     6     2     2     10\n",
      "22   order                    6     2     2     10\n",
      "23   silly                    6     2     2     10\n",
      "24   speech                   6     2     2     10\n",
      "25   sweet                    6     2     2     10\n",
      "26   california               5     2     2      9\n",
      "27   careful                  5     2     2      9\n",
      "28   drop                     5     2     2      9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def print_gloss_distribution(data_dir, min_per_split=2):\n",
    "    # Count structure: {gloss: {'train': x, 'val': y, 'test': z, 'total': t}}\n",
    "    gloss_counts = defaultdict(lambda: {'train': 0, 'val': 0, 'test': 0, 'total': 0})\n",
    "\n",
    "    # Count videos in each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(data_dir, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            continue\n",
    "\n",
    "        for gloss in os.listdir(split_path):\n",
    "            gloss_path = os.path.join(split_path, gloss)\n",
    "            if not os.path.isdir(gloss_path):\n",
    "                continue\n",
    "\n",
    "            video_files = [f for f in os.listdir(gloss_path) if f.endswith('.mp4')]\n",
    "            count = len(video_files)\n",
    "\n",
    "            gloss_counts[gloss][split] += count\n",
    "            gloss_counts[gloss]['total'] += count\n",
    "\n",
    "    # Filter glosses with enough videos in each split\n",
    "    filtered = {\n",
    "        gloss: counts for gloss, counts in gloss_counts.items()\n",
    "        if counts['train'] >= min_per_split and counts['val'] >= min_per_split and counts['test'] >= min_per_split\n",
    "    }\n",
    "\n",
    "    # Sort by total count descending\n",
    "    sorted_glosses = sorted(filtered.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "\n",
    "    # Print table with ranking\n",
    "    print(f\"{'#':<4} {'Gloss':<20} {'Train':>5} {'Val':>5} {'Test':>5} {'Total':>6}\")\n",
    "    print(\"-\" * 60)\n",
    "    for idx, (gloss, counts) in enumerate(sorted_glosses, start=1):\n",
    "        print(f\"{idx:<4} {gloss:<20} {counts['train']:>5} {counts['val']:>5} {counts['test']:>5} {counts['total']:>6}\")\n",
    "\n",
    "# Run the function\n",
    "print_gloss_distribution(data_dir='data', min_per_split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66392b6b",
   "metadata": {},
   "source": [
    "# 2. Fine-tune VideoMAE on a subset of 28 signs from the WLASL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f24196",
   "metadata": {},
   "source": [
    "## 2.1 Install dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision pytorchvideo transformers albumentations imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60b655",
   "metadata": {},
   "source": [
    "#### You can then count the number of total videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06833083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = pathlib.Path(\"data\")\n",
    "\n",
    "# Get all video file paths in train, val, test folders\n",
    "all_video_file_paths = list(dataset_root_path.glob(\"**/*.mp4\"))\n",
    "\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.mp4\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.mp4\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.mp4\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dbf6f",
   "metadata": {},
   "source": [
    "#### Derive the set of labels present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted({path.parent.name for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4368f15",
   "metadata": {},
   "source": [
    "## 2.2 Load a model to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd1ad0",
   "metadata": {},
   "source": [
    "#### Imports the relevant classes.\n",
    "\n",
    "- Sets the checkpoint/model name (`\"MCG-NJU/videomae-base\"`).\n",
    "\n",
    "- Loads the `VideoMAEImageProcessor` for preprocessing your videos.\n",
    "\n",
    "- Loads the `VideoMAEForVideoClassification` model pretrained on that checkpoint.\n",
    "\n",
    "- Passes the `label2id` and `id2label` mappings so the model knows your specific classes.\n",
    "\n",
    "- Uses `ignore_mismatched_sizes=True` so it can load the pretrained weights even if your classification head shape is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "hf_token = \"\"\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt, token=hf_token)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    token=hf_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc6ef2",
   "metadata": {},
   "source": [
    "## 2.3 Prepare the datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c84875",
   "metadata": {},
   "source": [
    "#### Import Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab329354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomRotation,\n",
    "    RandomAutocontrast,\n",
    "    RandomInvert,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "from albumentations import ElasticTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5624d",
   "metadata": {},
   "source": [
    "### Setting the input normalization, resolution, frame sampling, and clip duration so the `WLASL` videos are transformed into the exact shape/distribution that VideoMAE expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dfb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a584be4",
   "metadata": {},
   "source": [
    "### Define the dataset-specific transformations and the datasets respectively. Starting with the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81772eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class AddDistortion(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a video tensor (C, T, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, distortion=0.5):\n",
    "        super().__init__()\n",
    "        self.distortion = distortion\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert len(x.shape) == 4, \"video must have shape (C, T, H, W)\"\n",
    "        # Generate per-pixel Gaussian noise\n",
    "        noise = torch.randn_like(x) * self.distortion\n",
    "        return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0700fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms import RandomAutocontrast, RandomInvert\n",
    "\n",
    "def apply_with_prob(transform, p=0.3):\n",
    "    def wrapper(x):\n",
    "        if random.random() < p:\n",
    "            return transform(x)\n",
    "        return x\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to, antialias=True),\n",
    "\n",
    "            # Augmentations to reduce overfitting\n",
    "            RandomHorizontalFlip(p=0.4),\n",
    "            RandomRotation(degrees=10),\n",
    "            Lambda(lambda x: torch.tensor(ElasticTransform(alpha=30.0, sigma=4.0, alpha_affine=4.0)(x.permute(1, 2, 3, 0).numpy())).permute(3, 0, 1, 2)),\n",
    "            AddDistortion(0.1),\n",
    "\n",
    "            # RandomInvert & RandomAutocontrast with probability\n",
    "            Lambda(lambda x: apply_with_prob(RandomAutocontrast(p=1.0), p=0.2)(x)),\n",
    "            Lambda(lambda x: apply_with_prob(RandomInvert(p=1.0), p=0.3)(x)),\n",
    "        ])\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877726f",
   "metadata": {},
   "source": [
    "### Now apply the same to the validation and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to, antialias=True),\n",
    "        ])\n",
    "    )\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad418b",
   "metadata": {},
   "source": [
    "### Access the `num_videos` argument to know the number of videos in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ebc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b19539",
   "metadata": {},
   "source": [
    "## 2.4 Visualize the preprocessed video for better debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
