{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dc5a9d",
   "metadata": {},
   "source": [
    "# Sign-Language to Text Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408bf1e",
   "metadata": {},
   "source": [
    "## 1. Install all dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6af67",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdbc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install opencv-python\n",
    "%pip install transformers\n",
    "%pip install torchvision==0.16.2\n",
    "%pip install pytorchvideo==0.1.5\n",
    "%pip install imageio\n",
    "%pip install accelerate\n",
    "%pip install --upgrade mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257801ac",
   "metadata": {},
   "source": [
    "### 1.2 Restart the python kernal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d41928a",
   "metadata": {},
   "source": [
    "#### Please **restart the kernel** after running the cell above to apply newly installed packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7aae17",
   "metadata": {},
   "source": [
    "### 1.3 Load all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "import itertools\n",
    "import cv2\n",
    "import mlflow\n",
    "import accelerate\n",
    "import shutil\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from random import choice\n",
    "import torch\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import functional as F\n",
    "import pytorchvideo.data\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomRotation,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    "    RandomAutocontrast,\n",
    "    RandomInvert,\n",
    "    Grayscale,\n",
    "    ElasticTransform\n",
    ")\n",
    "from typing import Any, Callable, Dict, Optional, Type\n",
    "from pytorchvideo.data.clip_sampling import ClipSampler\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, matthews_corrcoef, confusion_matrix \n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28283d33",
   "metadata": {},
   "source": [
    "## 2. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60975227",
   "metadata": {},
   "source": [
    "### 2.1 Adjusting the format of `WLASL_v0.3.json`. Also select a subset of 28 glosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flat_dataset(wlasl_json_path, videos_dir, output_path):\n",
    "    # Only include these 29 selected glosses\n",
    "    selected_glosses = {\n",
    "        \"cousin\", \"deaf\", \"help\", \"call\", \"give\", \"take\", \"like\", \"laugh\",\n",
    "        \"order\", \"drop\", \"pizza\", \"candy\", \"shirt\", \"room\", \"bar\", \"language\",\n",
    "        \"speech\", \"cool\", \"silly\", \"sweet\", \"careful\", \"thin\", \"last\", \"soon\",\n",
    "        \"what\", \"california\", \"convince\", \"interest\"\n",
    "    }\n",
    "\n",
    "    with open(wlasl_json_path, \"r\") as f:\n",
    "        wlasl_data = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for entry in wlasl_data:\n",
    "        gloss = entry[\"gloss\"]\n",
    "\n",
    "        # Only include glosses from the selected list\n",
    "        if gloss not in selected_glosses:\n",
    "            continue\n",
    "\n",
    "        split = entry.get(\"split\", \"unknown\")\n",
    "\n",
    "        for instance in entry[\"instances\"]:\n",
    "            video_id = instance.get(\"video_id\")\n",
    "            video_filename = f\"{video_id}.mp4\"\n",
    "            video_path = videos_dir / video_filename\n",
    "\n",
    "            # Only include if the actual video file exists\n",
    "            if video_path.exists():\n",
    "                frame_start = instance.get(\"frame_start\")\n",
    "                frame_end = instance.get(\"frame_end\")\n",
    "                instance_split = instance.get(\"split\", split)\n",
    "\n",
    "                # Final video path in output JSON\n",
    "                relative_video_path = f\"data/{video_filename}\"\n",
    "\n",
    "                dataset.append({\n",
    "                    \"gloss\": gloss,\n",
    "                    \"video_path\": relative_video_path,\n",
    "                    \"frame_start\": frame_start,\n",
    "                    \"frame_end\": frame_end,\n",
    "                    \"split\": instance_split\n",
    "                })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac135b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd()\n",
    "wlasl_json_path = base_dir / \"WLASL_v0.3.json\"\n",
    "videos_dir = base_dir / \"Videos\"\n",
    "output_path = base_dir / \"WLASL_parsed_data_adjustedpath.json\"\n",
    "\n",
    "generate_flat_dataset(wlasl_json_path, videos_dir, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0b920",
   "metadata": {},
   "source": [
    "### 2.2 Restructure the files into `data/test`, `data/train` and `data/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f353be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open('WLASL_parsed_data_adjustedpath.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Base directory where the new folders will be created\n",
    "base_dir = 'data'\n",
    "moved_files = 0\n",
    "missing_files = 0\n",
    "\n",
    "for item in data:\n",
    "    # Use the full relative path from JSON (e.g., \"Videos/14894.mp4\")\n",
    "    current_path = os.path.normpath(item['video_path'])\n",
    "\n",
    "    if os.path.exists(current_path):\n",
    "        # Extract metadata\n",
    "        split = item['split']\n",
    "        gloss = item['gloss']\n",
    "\n",
    "        # Create destination directory structure\n",
    "        split_dir = os.path.join(base_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        gloss_dir = os.path.join(split_dir, gloss)\n",
    "        os.makedirs(gloss_dir, exist_ok=True)\n",
    "\n",
    "        # Get just the filename\n",
    "        filename = os.path.basename(current_path)\n",
    "\n",
    "        # Define the new destination path\n",
    "        new_path = os.path.join(gloss_dir, filename)\n",
    "\n",
    "        # Move the file\n",
    "        shutil.move(current_path, new_path)\n",
    "\n",
    "        moved_files += 1\n",
    "        print(f\"The video {current_path} is moved to {new_path}\")\n",
    "    else:\n",
    "        missing_files += 1\n",
    "        print(f\"The video {current_path} does not exist\")\n",
    "\n",
    "print(f\"Moved {moved_files} files and {missing_files} files are missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976e360",
   "metadata": {},
   "source": [
    "### 2.3 Display the video distribution of `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gloss_distribution(data_dir, min_per_split=2):\n",
    "    # Count structure: {gloss: {'train': x, 'val': y, 'test': z, 'total': t}}\n",
    "    gloss_counts = defaultdict(lambda: {'train': 0, 'val': 0, 'test': 0, 'total': 0})\n",
    "\n",
    "    # Count videos in each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(data_dir, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            continue\n",
    "\n",
    "        for gloss in os.listdir(split_path):\n",
    "            gloss_path = os.path.join(split_path, gloss)\n",
    "            if not os.path.isdir(gloss_path):\n",
    "                continue\n",
    "\n",
    "            video_files = [f for f in os.listdir(gloss_path) if f.endswith('.mp4')]\n",
    "            count = len(video_files)\n",
    "\n",
    "            gloss_counts[gloss][split] += count\n",
    "            gloss_counts[gloss]['total'] += count\n",
    "\n",
    "    # Filter glosses with enough videos in each split\n",
    "    filtered = {\n",
    "        gloss: counts for gloss, counts in gloss_counts.items()\n",
    "        if counts['train'] >= min_per_split and counts['val'] >= min_per_split and counts['test'] >= min_per_split\n",
    "    }\n",
    "\n",
    "    # Sort by total count descending\n",
    "    sorted_glosses = sorted(filtered.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "\n",
    "    # Print table with ranking\n",
    "    print(f\"{'#':<4} {'Gloss':<20} {'Train':>5} {'Val':>5} {'Test':>5} {'Total':>6}\")\n",
    "    print(\"-\" * 60)\n",
    "    for idx, (gloss, counts) in enumerate(sorted_glosses, start=1):\n",
    "        print(f\"{idx:<4} {gloss:<20} {counts['train']:>5} {counts['val']:>5} {counts['test']:>5} {counts['total']:>6}\")\n",
    "\n",
    "# Run the function\n",
    "print_gloss_distribution(data_dir='data', min_per_split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7ca10",
   "metadata": {},
   "source": [
    "## 3. Fine-tune `VideoMAE` on a subset of 28 signs from the `WLASL` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b4d5c",
   "metadata": {},
   "source": [
    "### 3.1 Counting the total number of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_path = pathlib.Path(\"data\")\n",
    "\n",
    "# Get all video file paths in train, val, test folders\n",
    "all_video_file_paths = list(dataset_root_path.glob(\"**/*.mp4\"))\n",
    "\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.mp4\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.mp4\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.mp4\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919930b",
   "metadata": {},
   "source": [
    "### 3.2 Derive the set of labels present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca779f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parent folder names (i.e., class labels) from video file paths\n",
    "class_labels = sorted({path.parent.name for path in all_video_file_paths})\n",
    "\n",
    "# Create label ↔ ID mappings\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Print class info\n",
    "print(f\"{len(class_labels)} unique classes:\")\n",
    "labels_str = ', '.join(label2id.keys())\n",
    "print('\\n'.join(textwrap.wrap(labels_str, width=150)))\n",
    "\n",
    "## Print the label2id and id2label mappings\n",
    "print(\"\\nLabel to ID mapping:\")\n",
    "print(label2id)\n",
    "\n",
    "print(\"\\nID to Label mapping:\")\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c486b39",
   "metadata": {},
   "source": [
    "### 3.3 Define functions to use for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06dbe2",
   "metadata": {},
   "source": [
    "#### 3.3.1 AddDistortion Class\n",
    "\n",
    "A PyTorch module that adds random Gaussian noise (distortion) to a video tensor.\n",
    "\n",
    "- **Purpose:** Introduces noise to video data for augmentation, improving model robustness.\n",
    "- **Input:** A video tensor of shape `(C, T, H, W)` — Channels, Time (frames), Height, Width.\n",
    "- **Output:** The input tensor with added distortion noise.\n",
    "\n",
    "> The noise is sampled from a normal distribution with mean 0 and a configurable standard deviation (`distortion`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1289ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddDistortion(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adds distortion to a video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, distortion=0.5):\n",
    "        super().__init__()\n",
    "        self.distortion = distortion\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): video tensor with shape (C, T, H, W).\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 4, \"video must have shape (C, T, H, W)\"\n",
    "        \n",
    "        # Create a new tensor with the same shape as x, filled with random values between -0.05 and 0.05\n",
    "        random_values = torch.rand_like(x) * 0 + np.random.normal(0, self.distortion)\n",
    "\n",
    "        # Add the random values to x\n",
    "        x = x + random_values\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc6e2f",
   "metadata": {},
   "source": [
    "#### 3.3.2 Count Files Per Folder\n",
    "\n",
    "This function counts the number of files in each subfolder of a given directory and groups folder names by their file counts.\n",
    "\n",
    "- **Input:** `dir_path` — path to the root directory containing label subfolders.\n",
    "- **Output:** Dictionary where keys are file counts and values are lists of folder names with that count.\n",
    "- **Purpose:** Helps analyze dataset distribution by showing how many files each class folder contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_per_folder(dir_path):\n",
    "    file_count = {}\n",
    "    file_count_grouped = {}\n",
    "    for dirpath, _, filenames in os.walk(dir_path):\n",
    "        if len(filenames) > 0:\n",
    "            label = os.path.basename(dirpath)\n",
    "            file_count[label] = len(filenames)\n",
    "    \n",
    "    for label, count in file_count.items():\n",
    "        if count not in file_count_grouped:\n",
    "            file_count_grouped[count] = []\n",
    "        file_count_grouped[count].append(label)\n",
    "    \n",
    "    file_count_grouped = dict(sorted(file_count_grouped.items()))\n",
    "    \n",
    "    return file_count_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5476e",
   "metadata": {},
   "source": [
    "#### 3.3.3 GIF Utilities for Video Tensors\n",
    "\n",
    "These functions convert a normalized video tensor into a displayable GIF:\n",
    "\n",
    "- **`unnormalize_img(img, mean, std)`**  \n",
    "  Reverses normalization by applying mean and std, then scales pixel values back to the [0, 255] range.\n",
    "\n",
    "- **`create_gif(video_tensor, mean, std, filename)`**  \n",
    "  Converts a video tensor of shape `(num_frames, channels, height, width)` into a GIF by unnormalizing each frame and saving it.\n",
    "\n",
    "- **`display_gif(video_tensor, mean, std, gif_name)`**  \n",
    "  Prepares and displays the GIF directly in a notebook by calling `create_gif` and rendering the saved file.\n",
    "\n",
    "> Useful for visualizing video clips during preprocessing or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6aa14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_img(img, mean, std):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, mean, std, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy(), mean, std)\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, mean, std, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, mean, std, gif_name)\n",
    "    return Image(filename=gif_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aac8c7",
   "metadata": {},
   "source": [
    "#### 3.3.4 Explanation of Functions\n",
    "\n",
    "##### `compute_metrics(eval_pred)`\n",
    "- Calculates multiple evaluation metrics for model predictions:\n",
    "  - **Accuracy, F1-score, Precision, Recall** (weighted averages).\n",
    "  - Computes **confusion matrix** and per-class precision and recall using `classification_report`.\n",
    "- Returns a dictionary containing all metrics, including precision and recall for each class.\n",
    "\n",
    "##### `collate_fn(examples)`\n",
    "- Prepares a batch of video samples for the model.\n",
    "- Converts a list of examples into:\n",
    "  - `pixel_values`: stacked video tensors permuted to shape `(batch, num_frames, num_channels, height, width)`.\n",
    "  - `labels`: tensor of corresponding labels.\n",
    "\n",
    "##### `run_inference(model, video)`\n",
    "- Performs inference on a single video tensor.\n",
    "- Expects video tensor shape `(num_frames, num_channels, height, width)`.\n",
    "- Permutes and batches the input, sends it to the device (CPU/GPU).\n",
    "- Returns raw logits output by the model for further processing (e.g., applying softmax or argmax).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Load the metrics\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    precision = evaluate.load(\"precision\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    \n",
    "    # Compute predictions\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "\n",
    "    # Compute the metrics\n",
    "    accuracy_result = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    f1_result = f1.compute(predictions=predictions, references=eval_pred.label_ids, average='weighted')\n",
    "    precision_result = precision.compute(predictions=predictions, references=eval_pred.label_ids, average='weighted', zero_division=0)\n",
    "    recall_result = recall.compute(predictions=predictions, references=eval_pred.label_ids, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    confmat = confusion_matrix(eval_pred.label_ids, predictions)\n",
    "    \n",
    "    # Compute precision and recall per class\n",
    "    report = classification_report(eval_pred.label_ids, predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    label_values = [int(key) for key in report.keys() if key.isdigit()]\n",
    "    precision_per_class_dict = {f'precision_{id2label[label]}': report[str(label)]['precision'] for label in label_values}\n",
    "    recall_per_class_dict = {f'recall_{id2label[label]}': report[str(label)]['recall'] for label in label_values}\n",
    "\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        \"accuracy\": accuracy_result['accuracy'],\n",
    "        \"f1\": f1_result['f1'],\n",
    "        \"precision\": precision_result['precision'],\n",
    "        \"recall\": recall_result['recall'],\n",
    "        # \"confusionmatrix\": confmat,\n",
    "        **precision_per_class_dict,\n",
    "        **recall_per_class_dict\n",
    "    }\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "def run_inference(model, video):\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    permuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "    inputs = {\n",
    "        \"pixel_values\": permuted_sample_test_video.unsqueeze(0)\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7f40a",
   "metadata": {},
   "source": [
    "### 3.4 Load a model to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd44a18",
   "metadata": {},
   "source": [
    "##### Imports the relevant classes.\n",
    "\n",
    "- Sets the checkpoint/model name (`\"MCG-NJU/videomae-base\"`).\n",
    "\n",
    "- Loads the `VideoMAEImageProcessor` for preprocessing your videos.\n",
    "\n",
    "- Loads the `VideoMAEForVideoClassification` model pretrained on that checkpoint.\n",
    "\n",
    "- Passes the `label2id` and `id2label` mappings so the model knows your specific classes.\n",
    "\n",
    "- Uses `ignore_mismatched_sizes=True` so it can load the pretrained weights even if your classification head shape is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load the default pretrained model\n",
    "model_ckpt = \"MCG-NJU/videomae-huge-finetuned-kinetics\"\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    ignore_mismatched_sizes=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d3175",
   "metadata": {},
   "source": [
    "### 3.5  Prepare the datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802f3b7",
   "metadata": {},
   "source": [
    "#### 3.5.1 Video Preprocessing Parameters Setup\n",
    "\n",
    "- **Mean and Standard Deviation:**  \n",
    "  Retrieved from the `image_processor` to normalize video frames during preprocessing.\n",
    "\n",
    "- **Resize Dimensions:**  \n",
    "  Determines target height and width for resizing videos.  \n",
    "  - If `shortest_edge` is specified in the processor's size, it is used for both height and width (square resize).  \n",
    "  - Otherwise, height and width are set individually.\n",
    "\n",
    "- **Frame Sampling Parameters:**  \n",
    "  - `num_frames_to_sample`: Number of frames the model expects per video clip.  \n",
    "  - `sample_rate`: Frame sampling rate (e.g., every 3rd frame).  \n",
    "  - `fps`: Frames per second of the original videos.\n",
    "\n",
    "- **Clip Duration Calculation:**  \n",
    "  Computes the total duration of the video clip to sample based on the number of frames, sample rate, and fps.  \n",
    "  This ensures consistent clip length matching model input requirements.\n",
    "\n",
    "- **Output:**  \n",
    "  Prints out the final height, width, number of frames, and clip duration for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 3\n",
    "fps = 28\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "print(f\"\"\"the height of the video is {height}, the width is {width}, \n",
    "the number of frames is {num_frames_to_sample}, the clip duration is {clip_duration}.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd9e2ce",
   "metadata": {},
   "source": [
    "#### 3.5.2 Extracting Video Properties from MP4 Files\n",
    "\n",
    "- **Finding Video Files:**  \n",
    "  Uses `glob.iglob` with recursive search to find all `.mp4` files under `folder_path`.\n",
    "\n",
    "- **Selecting Files:**  \n",
    "  `itertools.islice` limits the search to the first 10 `.mp4` files found.\n",
    "\n",
    "- **Reading Video Metadata:**  \n",
    "  For each video:  \n",
    "  - Opens the video with OpenCV's `VideoCapture`.  \n",
    "  - Retrieves the video's width, height, and frames per second (FPS) using OpenCV properties.\n",
    "\n",
    "- **Output:**  \n",
    "  Prints out the resolution and frame rate for each sampled video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use itertools.islice() to get the first 10 .mp4 files\n",
    "video_paths = list(itertools.islice(all_video_file_paths, 10))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the width, height and frame rate of the video\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    print(f\"For video {video_path}, the width is {width}, the height is {height}, the frame rate is {fps}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a7a27",
   "metadata": {},
   "source": [
    "### 3.6  Define the dataset-specific transformations and the datasets respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d9da7",
   "metadata": {},
   "source": [
    "#### 3.6.1 Starting with the training set:\n",
    "\n",
    "- **Custom frame-wise augmentation:**  \n",
    "  Using `RandomTransformCustom` to apply transforms like autocontrast and invert randomly to individual video frames.\n",
    "\n",
    "- **Training transform pipeline:**  \n",
    "  Combines temporal subsampling, normalization, resizing, standard augmentations (flip, rotation, elastic transform), distortion, and the custom frame-wise transforms.\n",
    "\n",
    "- **Training dataset:**  \n",
    "  Created using `pytorchvideo.data.Ucf101` with the training transform pipeline, random clip sampling, and no audio decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2552d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generalized custom transform for applying any transform with a given probability\n",
    "class RandomTransformCustom(torch.nn.Module):\n",
    "    def __init__(self, transform, p=0.3):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.transform = transform  # Pass any transform object\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming input shape is (C, T, H, W) where:\n",
    "        # C = channels (1 or 3 typically)\n",
    "        # T = number of frames\n",
    "        # H = height of frames\n",
    "        # W = width of frames\n",
    "        c, t, h, w = x.shape\n",
    "        for i in range(t):  # Loop over frames (temporal dimension)\n",
    "            if random.random() < self.p:\n",
    "                x[:, i, :, :] = self.transform(x[:, i, :, :])  # Apply transform to each frame\n",
    "        return x\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    # same arguments as test set\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to, antialias=True),\n",
    "                    \n",
    "                    # additional noise to avoid overfitting\n",
    "                    RandomHorizontalFlip(p=0.4),\n",
    "                    RandomRotation(degrees=10),\n",
    "                    ElasticTransform(alpha=30.0),\n",
    "                    AddDistortion(0.1),\n",
    "\n",
    "                    # Use generalized RandomTransformCustom for both RandomInvert and RandomAutocontrast\n",
    "                    RandomTransformCustom(RandomAutocontrast(p=1.0), p=0.2),  \n",
    "                    RandomTransformCustom(RandomInvert(p=1.0), p=0.3),        \n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(videos_dir, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d349d5d",
   "metadata": {},
   "source": [
    "#### 3.6.2 The same sequence of workflow can be applied to the test dataset:\n",
    "\n",
    "- **Validation/test transform pipeline:**  \n",
    "  A simpler transform pipeline that includes temporal subsampling, normalization, and resizing—no augmentations, to evaluate model performance reliably.\n",
    "\n",
    "- **Test dataset:**  \n",
    "  Created with `pytorchvideo.data.Ucf101` using the validation transform pipeline, random clip sampling, and no audio decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to, antialias=True),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(videos_dir, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87051c",
   "metadata": {},
   "source": [
    "#### 3.6.3 Count the number of video files per class (folder) in both the training and test directories using the `count_files_per_folder` function:\n",
    "\n",
    "- `train_count_labels`: Dictionary grouping classes by the number of files in the training set.\n",
    "- `test_count_labels`: Dictionary grouping classes by the number of files in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_labels = count_files_per_folder(os.path.join(videos_dir, \"train\"))\n",
    "test_count_labels = count_files_per_folder(os.path.join(videos_dir, \"test\"))\n",
    "\n",
    "print(\"In the train folder, the number of videos per label are:\")\n",
    "for k, v in train_count_labels.items():\n",
    "    print(f\"{len(v)} labels with {k} video(s): {v}\")\n",
    "    \n",
    "print(\"In the test folder, the number of videos per label are:\")\n",
    "for k, v in test_count_labels.items():\n",
    "    print(f\"{len(v)} labels with {k} video(s): {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23f47d",
   "metadata": {},
   "source": [
    "### 3.7 Visualize the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor, mean, std, \"vid_example.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91436b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = next(iter(test_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor, mean, std, \"val_example.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb779d7",
   "metadata": {},
   "source": [
    "## 3.8 Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19ad78",
   "metadata": {},
   "source": [
    "#### 3.8.1 Check NVIDIA GPU information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c7206",
   "metadata": {},
   "source": [
    "#### 3.8.2 Check the number of videos in the training dataset\n",
    "- `train_dataset.num_videos`:  \n",
    "  Returns the total number of video samples in the training dataset.  \n",
    "  This is useful for verifying dataset size or calculating steps per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.num_videos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fd4ab",
   "metadata": {},
   "source": [
    "#### 3.8.3 Most of the training arguments below are self-explanatory, but one particularly important one is:\n",
    "\n",
    "- `remove_unused_columns=False`:  \n",
    "  This prevents the `Trainer` from automatically dropping input features not explicitly used by the model's `forward` method.  \n",
    "  In this case, we **need the `'video'` key** to generate `pixel_values` — a required input for the `VideoMAE` model.  \n",
    "  If `remove_unused_columns` were left as `True`, those unused fields (like `'video'`) would be removed before reaching the model.\n",
    "\n",
    "Other key parameters:\n",
    "- `output_dir`: Where to save the model.\n",
    "- `learning_rate`: Fine-tuned for small updates.\n",
    "- `warmup_ratio`: Helps with training stability.\n",
    "- `max_steps`: Total number of training steps (based on dataset size, batch size, and epochs).\n",
    "- `logging_steps`, `save_steps`, etc.: Control how often logs and checkpoints are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "training_start_moment = datetime.now().isoformat(timespec='hours')\n",
    "new_model_name = f\"{model_name}-sign_finetuned-{training_start_moment}\"\n",
    "\n",
    "output_dir = os.path.join(\"Models\", new_model_name)\n",
    "num_epochs = 100 \n",
    "batch_size = 16\n",
    "\n",
    "max_steps = (train_dataset.num_videos // batch_size) * num_epochs\n",
    "logging_steps = 10 if max_steps >= 10 else 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    learning_rate=0.5e-5,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.2,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    max_steps=max_steps,\n",
    "    overwrite_output_dir=True,\n",
    "    weight_decay=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856dbb1",
   "metadata": {},
   "source": [
    "#### 3.8.4 `max_steps` Calculation\n",
    "\n",
    "This line prints the total number of training steps (`max_steps`) that will be executed during the fine-tuning process. It is computed based on:\n",
    "\n",
    "- the size of the training dataset,\n",
    "- the batch size,\n",
    "- and the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4daef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"max_steps: {args.max_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185cb0a",
   "metadata": {},
   "source": [
    "#### 3.8.5 Check Available CUDA Devices\n",
    "\n",
    "This line prints the number of CUDA-enabled GPUs (e.g., NVIDIA GPUs) available on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74154b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d3968",
   "metadata": {},
   "source": [
    "#### 3.8.6 Initialize the Trainer\n",
    "\n",
    "This creates a `Trainer` object from the Hugging Face `transformers` library, which handles the training and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c0620",
   "metadata": {},
   "source": [
    "#### 3.8.7 Check if torch can use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba906563",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80e64c",
   "metadata": {},
   "source": [
    "#### 3.8.8 If there are a lot of labels in the dataset, params need to be flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"true\"\n",
    "\n",
    "with mlflow.start_run(run_name=new_model_name):\n",
    "    train_results = trainer.train()\n",
    "    mlflow.log_param(\"label2id\", json.dumps(label2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5de46",
   "metadata": {},
   "source": [
    "#### 3.8.9 Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2467c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.path.join(\"Models\", new_model_name, \"best_model\")\n",
    "\n",
    "os.makedirs(model_save_path, exist_ok=True)  # create folder if needed\n",
    "\n",
    "# Save the model and image processor locally\n",
    "model.save_pretrained(model_save_path)\n",
    "image_processor.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967444e",
   "metadata": {},
   "source": [
    "### 3.9 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b491929",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3358e",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0e8b4",
   "metadata": {},
   "source": [
    "### 4.1 Run Inference on Multiple Sample Test Videos\n",
    "\n",
    "This code snippet runs inference on 5 sample videos randomly selected from the test dataset. For each sample, it prints the predicted and actual labels and displays the video as a GIF.\n",
    "\n",
    "- The `run_inference` function processes each video tensor and returns the model's raw output logits.\n",
    "- The predicted class is determined by taking the index of the highest logit.\n",
    "- The actual class label is retrieved from the `id2label` dictionary.\n",
    "- Each video is saved and displayed as a GIF for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a527740",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    sample_test_video = choice(test_dataset)\n",
    "    \n",
    "    logits = run_inference(model, sample_test_video[\"video\"])\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Predicted class: {model.config.id2label[predicted_class_idx]}\")\n",
    "    print(f\"  Real class: {id2label[sample_test_video['label']]}\")\n",
    "    \n",
    "    video_tensor = sample_test_video[\"video\"]\n",
    "    display_gif(video_tensor, mean, std, f\"example_{i}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d577b",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4aa70c",
   "metadata": {},
   "source": [
    "### 5.1 Remove all .gif files in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320696ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.getcwd()):\n",
    "    if file.endswith(\".gif\"):\n",
    "        os.remove(file)\n",
    "        print(f\"The file {file} has been removed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
